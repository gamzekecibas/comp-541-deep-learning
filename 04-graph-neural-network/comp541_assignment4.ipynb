{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn9aNItt4okk"
   },
   "source": [
    "# Koç University, Deep Learning Course (COMP541) <br/> Assignment 4: Graph Convolution Networks\n",
    "\n",
    "In this assignment, you will implement the vanilla version of Graph Convolution\n",
    "Networks (GCN) [Kipf and Welling \\(2016\\)](https://arxiv.org/abs/1609.02907) and Graph Attention Networks (GAT) [Veličković, et al.\n",
    "\\(2018\\)](https://openreview.net/forum?id=rJXMpikCZ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ab_-ToD6EWN"
   },
   "source": [
    "## Background\n",
    "### Basics of GCN\n",
    "Recall from the lectures, the goal of a GCN is to learn a function of signals features on a graph $G = (V, E)$, which takes as inputs:\n",
    "1. he input features of each node, $x_i ∈ R^F$ (in matrix form: $X ∈ R^{|V |×F}$ )\n",
    "2. some information about the graph structure, typically the adjacency matrix $A$\n",
    "\n",
    "Each convolutional layer can be written as $H^{(l+1)} = f(H^{(l)}, A)$ for some function $f$. The function $f$ we are using for this assignment is in the form of $f(H^{(l)}, A) = σ(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}H^{(l)}W^{(l)})$, where $\\hat{A} = A + I$ and $\\hat{D}$ is the diagonal node degree matrix ($D^{-1}\\hat{A}$ normalizes $\\hat{A}$ such that all rows sum to one). Let $\\tilde{A} = \\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}$. The GCN we will implement takes two convolution layers, $Z = f(X, A) = softmax(\\tilde{A}~.~Dropout(ReLU(\\tilde{A}XW^{(0)}))~.W^{(1)})$\n",
    "\n",
    "### Basics of GAT\n",
    "Graph Attention Network (GAT) is a novel convolution-style neural network. It operates on graph-structured data and leverages masked self-attentional layers. In this assignment, we will implement the graph attention layer.\n",
    "\n",
    "### Dataset\n",
    "The dataset we used for this assignment is Cora ([Sen et al. \\(2008\\)](http://www.cs.iit.edu/~ml/pdfs/sen-aimag08.pdf)). Cora is one of standard citation network benchmark dataset (just like MNIST dataset for computer vision tasks). It that consists of 2708 scientific publications and 5429 links. Each publication is classified into one of 7 classes. Each publication is described by a word vector (length 1433) that indicates the absence/presence of the corresponding word. This is used as the features of each node for our experiment. The task is to perform node classification (predict which class each node belongs to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTsdgLCErn4Z"
   },
   "source": [
    "## Experiments\n",
    "Experiments:\n",
    "Open GCN notebook on Colab and implement the following parts.\n",
    "1. [1 pt] Implementation of Graph Convolution Layer: Complete the code for `GraphConvolution` Class\n",
    "2. [1 pt] Implementation of Graph Convolution Network: Complete the code for `GCN` Class\n",
    "3. [0.5 pt] Train your Graph Convolution Network: After implementing the required classes, now you can train your GCN. You can play with the hyperparameters in args.\n",
    "4. [2 pt] Implementation of Graph Attention Layer: Complete the code for `GraphAttentionLayer` Class\n",
    "5. [0.5 pt] Train your Graph Convolution Network: After implementing the required classes, now you can train your GAT. You can play with the hyperparameters in args.\n",
    "6. [0.5 pt] Compare your models: Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it’s the case (in 1-2 sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16bNiYqaqjki"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImLCXm8IsSS2"
   },
   "source": [
    "# Download the Cora data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRN47p1SKRgP",
    "outputId": "0b18d0f9-9918-4f23-96f7-a1db9514bd97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-08 00:29:24--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
      "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
      "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168052 (164K) [application/x-gzip]\n",
      "Saving to: ‘cora.tgz.9’\n",
      "\n",
      "cora.tgz.9          100%[===================>] 164,11K   283KB/s    in 0,6s    \n",
      "\n",
      "2023-01-08 00:29:26 (283 KB/s) - ‘cora.tgz.9’ saved [168052/168052]\n",
      "\n",
      "x cora/\n",
      "x cora/README\n",
      "x cora/cora.cites\n",
      "x cora/cora.content\n"
     ]
    }
   ],
   "source": [
    "! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "! tar -zxvf cora.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXIYzURA4OKg"
   },
   "source": [
    "# import modules and set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uJQYMX02_z0M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "seed = 0\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgOv1h7YsK-5"
   },
   "source": [
    "# Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kXPHN61i9keB"
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"./content/cora/\", dataset=\"cora\", training_samples=140):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = normalize_adj(adj)\n",
    "\n",
    "    # Random indexes\n",
    "    idx_rand = torch.randperm(len(labels))\n",
    "    # Nodes for training\n",
    "    idx_train = idx_rand[:training_samples]\n",
    "    # Nodes for validation\n",
    "    idx_val= idx_rand[training_samples:]\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"symmetric normalization\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzCZVd1JsbHr"
   },
   "source": [
    "## check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlsKjMKx8_b7",
    "outputId": "f216ba4c-ae9b-49ff-e606-2770c35c57bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxrv21rLnpiZ",
    "outputId": "5c27e472-3b88-4a83-e84a-4ca040e2fa85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1667, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2500]])\n",
      "torch.Size([2708, 2708])\n"
     ]
    }
   ],
   "source": [
    "print(adj)\n",
    "print(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWrDf0iWnpqV",
    "outputId": "94cd7295-d969-4b03-d787-a574e08f8d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "print(features)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUkt2JJdsuA2",
    "outputId": "7775b5ff-1615-44a6-9046-3bdfce5ae87b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 4,  ..., 1, 0, 2])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "2708\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(labels.unique())\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iGP18jNAs1Gp",
    "outputId": "9bebdbe6-1e7f-4c63-8fb1-4a6a5eecc416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "2568\n"
     ]
    }
   ],
   "source": [
    "print(len(idx_train))\n",
    "print(len(idx_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHqIcfH-vIic"
   },
   "source": [
    "# Vanilla GCN for node classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f48tylWyjLPE"
   },
   "source": [
    "## Define Graph Convolution layer (Your Task)\n",
    "\n",
    "This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n",
    "1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n",
    "2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M-fU8L7f41VZ"
   },
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    A Graph Convolution Layer (GCN)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        \"\"\"\n",
    "        * `in_features`, $F$, is the number of input features per node\n",
    "        * `out_features`, $F'$, is the number of output features per node\n",
    "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
    "        \"\"\"\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n",
    "        # hint: use nn.Linear()\n",
    "        ############ Your code here ###################################\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "        \n",
    "\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n",
    "        # to sum over neighbouring nodes )\n",
    "        # hint: use the linear layer you declared above. \n",
    "        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n",
    "        #       adjacency matrix\n",
    "        ############ Your code here ###################################\n",
    "        x = self.linear(input)\n",
    "        x = torch.spmm(adj, x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "        ###############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxBELCxkjF6F"
   },
   "source": [
    "## Define GCN (Your Task)\n",
    "\n",
    "you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HtVr2cN8jD5t"
   },
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    '''\n",
    "    A two-layer GCN\n",
    "    '''\n",
    "    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n",
    "        \"\"\"\n",
    "        * `nfeat`, is the number of input features per node of the first layer\n",
    "        * `n_hidden`, number of hidden units\n",
    "        * `n_classes`, total number of classes for classification\n",
    "        * `dropout`, the dropout ratio\n",
    "        * `bias`, whether to include the bias term in the linear layer. Default=True\n",
    "        \"\"\"\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "        # TODO: Initialization\n",
    "        # (1) 2 GraphConvolution() layers. \n",
    "        # (2) 1 Dropout layer\n",
    "        # (3) 1 activation function: ReLU()\n",
    "        ############ Your code here ###################################\n",
    "        self.gc1 = GraphConvolution(nfeat, n_hidden, bias=bias)\n",
    "        self.gc2 = GraphConvolution(n_hidden, n_classes, bias=bias)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # TODO: the input will pass through the first graph convolution layer, \n",
    "        # the activation function, the dropout layer, then the second graph \n",
    "        # convolution layer. No activation function for the \n",
    "        # last layer. Return the logits. \n",
    "        ############ Your code here ###################################\n",
    "        x = self.relu(self.gc1(x, adj))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim = 1)\n",
    "\n",
    "        ###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX1d9F1G508r"
   },
   "source": [
    "## define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HyhqJ39OCzNN"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXsdid6C5K1c"
   },
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bjlYeoFPFAWm"
   },
   "outputs": [],
   "source": [
    "args = {\"training_samples\": 140,\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"hidden\": 16,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bias\": True, \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Qbx0uc-9G5vs"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = criterion(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = criterion(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = criterion(output[idx_val], labels[idx_val])\n",
    "    acc_test = accuracy(output[idx_val], labels[idx_val])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjNiui83FYBr",
    "outputId": "ecf2ac6b-176c-43ba-a0a5-2a7206d0d27a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nfeat=features.shape[1],\n",
    "            n_hidden=args[\"hidden\"],\n",
    "            n_classes=labels.max().item() + 1,\n",
    "            dropout=args[\"dropout\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "\n",
    "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
    "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W6tqqj16iz-"
   },
   "source": [
    "## training Vanilla GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "WSjUYJPSlnOU",
    "outputId": "8ccce9f6-9e9a-42a3-c5c4-f116705ad57e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9387 acc_train: 0.1143 loss_val: 1.9272 acc_val: 0.1597 time: 0.0747s\n",
      "Epoch: 0002 loss_train: 1.9317 acc_train: 0.1143 loss_val: 1.9225 acc_val: 0.1597 time: 0.0538s\n",
      "Epoch: 0003 loss_train: 1.9243 acc_train: 0.1143 loss_val: 1.9170 acc_val: 0.1671 time: 0.0535s\n",
      "Epoch: 0004 loss_train: 1.9156 acc_train: 0.1857 loss_val: 1.9110 acc_val: 0.2208 time: 0.0538s\n",
      "Epoch: 0005 loss_train: 1.9066 acc_train: 0.2286 loss_val: 1.9047 acc_val: 0.1347 time: 0.0539s\n",
      "Epoch: 0006 loss_train: 1.8991 acc_train: 0.1857 loss_val: 1.8981 acc_val: 0.1312 time: 0.0549s\n",
      "Epoch: 0007 loss_train: 1.8898 acc_train: 0.2000 loss_val: 1.8913 acc_val: 0.1896 time: 0.0545s\n",
      "Epoch: 0008 loss_train: 1.8785 acc_train: 0.3714 loss_val: 1.8843 acc_val: 0.3727 time: 0.0541s\n",
      "Epoch: 0009 loss_train: 1.8699 acc_train: 0.3571 loss_val: 1.8770 acc_val: 0.3551 time: 0.0540s\n",
      "Epoch: 0010 loss_train: 1.8570 acc_train: 0.4000 loss_val: 1.8694 acc_val: 0.3131 time: 0.0551s\n",
      "Epoch: 0011 loss_train: 1.8498 acc_train: 0.3786 loss_val: 1.8616 acc_val: 0.3033 time: 0.0535s\n",
      "Epoch: 0012 loss_train: 1.8437 acc_train: 0.3500 loss_val: 1.8537 acc_val: 0.3018 time: 0.0531s\n",
      "Epoch: 0013 loss_train: 1.8253 acc_train: 0.3357 loss_val: 1.8458 acc_val: 0.3014 time: 0.0530s\n",
      "Epoch: 0014 loss_train: 1.8101 acc_train: 0.3357 loss_val: 1.8379 acc_val: 0.3014 time: 0.0541s\n",
      "Epoch: 0015 loss_train: 1.7982 acc_train: 0.3286 loss_val: 1.8299 acc_val: 0.3010 time: 0.0529s\n",
      "Epoch: 0016 loss_train: 1.7853 acc_train: 0.3286 loss_val: 1.8220 acc_val: 0.3010 time: 0.0529s\n",
      "Epoch: 0017 loss_train: 1.7726 acc_train: 0.3571 loss_val: 1.8144 acc_val: 0.3010 time: 0.0533s\n",
      "Epoch: 0018 loss_train: 1.7699 acc_train: 0.3357 loss_val: 1.8071 acc_val: 0.3010 time: 0.0545s\n",
      "Epoch: 0019 loss_train: 1.7470 acc_train: 0.3214 loss_val: 1.8002 acc_val: 0.3010 time: 0.0551s\n",
      "Epoch: 0020 loss_train: 1.7401 acc_train: 0.3214 loss_val: 1.7940 acc_val: 0.3010 time: 0.0537s\n",
      "Epoch: 0021 loss_train: 1.7357 acc_train: 0.3214 loss_val: 1.7881 acc_val: 0.3010 time: 0.0539s\n",
      "Epoch: 0022 loss_train: 1.7165 acc_train: 0.3214 loss_val: 1.7827 acc_val: 0.3010 time: 0.0557s\n",
      "Epoch: 0023 loss_train: 1.7068 acc_train: 0.3214 loss_val: 1.7776 acc_val: 0.3010 time: 0.0530s\n",
      "Epoch: 0024 loss_train: 1.6761 acc_train: 0.3286 loss_val: 1.7728 acc_val: 0.3010 time: 0.0528s\n",
      "Epoch: 0025 loss_train: 1.6755 acc_train: 0.3286 loss_val: 1.7680 acc_val: 0.3010 time: 0.0529s\n",
      "Epoch: 0026 loss_train: 1.6797 acc_train: 0.3214 loss_val: 1.7632 acc_val: 0.3010 time: 0.0546s\n",
      "Epoch: 0027 loss_train: 1.6821 acc_train: 0.3214 loss_val: 1.7580 acc_val: 0.3010 time: 0.0535s\n",
      "Epoch: 0028 loss_train: 1.6489 acc_train: 0.3214 loss_val: 1.7522 acc_val: 0.3010 time: 0.0537s\n",
      "Epoch: 0029 loss_train: 1.6361 acc_train: 0.3214 loss_val: 1.7457 acc_val: 0.3010 time: 0.0534s\n",
      "Epoch: 0030 loss_train: 1.6336 acc_train: 0.3286 loss_val: 1.7384 acc_val: 0.3010 time: 0.0542s\n",
      "Epoch: 0031 loss_train: 1.6313 acc_train: 0.3286 loss_val: 1.7302 acc_val: 0.3010 time: 0.0531s\n",
      "Epoch: 0032 loss_train: 1.5999 acc_train: 0.3357 loss_val: 1.7214 acc_val: 0.3014 time: 0.0527s\n",
      "Epoch: 0033 loss_train: 1.5918 acc_train: 0.3500 loss_val: 1.7120 acc_val: 0.3018 time: 0.0527s\n",
      "Epoch: 0034 loss_train: 1.5582 acc_train: 0.3429 loss_val: 1.7022 acc_val: 0.3022 time: 0.0561s\n",
      "Epoch: 0035 loss_train: 1.5664 acc_train: 0.3500 loss_val: 1.6921 acc_val: 0.3022 time: 0.0536s\n",
      "Epoch: 0036 loss_train: 1.5537 acc_train: 0.3357 loss_val: 1.6819 acc_val: 0.3030 time: 0.0535s\n",
      "Epoch: 0037 loss_train: 1.5232 acc_train: 0.4000 loss_val: 1.6713 acc_val: 0.3057 time: 0.0529s\n",
      "Epoch: 0038 loss_train: 1.5163 acc_train: 0.3786 loss_val: 1.6606 acc_val: 0.3107 time: 0.0540s\n",
      "Epoch: 0039 loss_train: 1.5190 acc_train: 0.4071 loss_val: 1.6496 acc_val: 0.3217 time: 0.0528s\n",
      "Epoch: 0040 loss_train: 1.4982 acc_train: 0.4500 loss_val: 1.6383 acc_val: 0.3318 time: 0.0537s\n",
      "Epoch: 0041 loss_train: 1.5015 acc_train: 0.4214 loss_val: 1.6267 acc_val: 0.3431 time: 0.0536s\n",
      "Epoch: 0042 loss_train: 1.4785 acc_train: 0.5000 loss_val: 1.6152 acc_val: 0.3532 time: 0.0549s\n",
      "Epoch: 0043 loss_train: 1.4484 acc_train: 0.5071 loss_val: 1.6036 acc_val: 0.3621 time: 0.0533s\n",
      "Epoch: 0044 loss_train: 1.4655 acc_train: 0.4571 loss_val: 1.5919 acc_val: 0.3746 time: 0.0527s\n",
      "Epoch: 0045 loss_train: 1.3999 acc_train: 0.5357 loss_val: 1.5807 acc_val: 0.3840 time: 0.0527s\n",
      "Epoch: 0046 loss_train: 1.4011 acc_train: 0.5429 loss_val: 1.5696 acc_val: 0.3941 time: 0.0542s\n",
      "Epoch: 0047 loss_train: 1.3896 acc_train: 0.5357 loss_val: 1.5586 acc_val: 0.3991 time: 0.0529s\n",
      "Epoch: 0048 loss_train: 1.3822 acc_train: 0.5357 loss_val: 1.5477 acc_val: 0.4124 time: 0.0526s\n",
      "Epoch: 0049 loss_train: 1.3419 acc_train: 0.5500 loss_val: 1.5370 acc_val: 0.4213 time: 0.0532s\n",
      "Epoch: 0050 loss_train: 1.3299 acc_train: 0.5643 loss_val: 1.5263 acc_val: 0.4322 time: 0.0539s\n",
      "Epoch: 0051 loss_train: 1.3316 acc_train: 0.5786 loss_val: 1.5153 acc_val: 0.4400 time: 0.0536s\n",
      "Epoch: 0052 loss_train: 1.3169 acc_train: 0.5714 loss_val: 1.5043 acc_val: 0.4474 time: 0.0526s\n",
      "Epoch: 0053 loss_train: 1.3122 acc_train: 0.5857 loss_val: 1.4932 acc_val: 0.4579 time: 0.0526s\n",
      "Epoch: 0054 loss_train: 1.2733 acc_train: 0.6000 loss_val: 1.4823 acc_val: 0.4650 time: 0.0539s\n",
      "Epoch: 0055 loss_train: 1.2716 acc_train: 0.5786 loss_val: 1.4710 acc_val: 0.4759 time: 0.0527s\n",
      "Epoch: 0056 loss_train: 1.2419 acc_train: 0.6286 loss_val: 1.4595 acc_val: 0.4856 time: 0.0525s\n",
      "Epoch: 0057 loss_train: 1.2427 acc_train: 0.6071 loss_val: 1.4481 acc_val: 0.4949 time: 0.0526s\n",
      "Epoch: 0058 loss_train: 1.2116 acc_train: 0.6571 loss_val: 1.4366 acc_val: 0.5039 time: 0.0538s\n",
      "Epoch: 0059 loss_train: 1.2372 acc_train: 0.6786 loss_val: 1.4252 acc_val: 0.5152 time: 0.0526s\n",
      "Epoch: 0060 loss_train: 1.1936 acc_train: 0.6643 loss_val: 1.4142 acc_val: 0.5261 time: 0.0525s\n",
      "Epoch: 0061 loss_train: 1.1288 acc_train: 0.7071 loss_val: 1.4034 acc_val: 0.5350 time: 0.0526s\n",
      "Epoch: 0062 loss_train: 1.1493 acc_train: 0.6929 loss_val: 1.3927 acc_val: 0.5444 time: 0.0539s\n",
      "Epoch: 0063 loss_train: 1.1522 acc_train: 0.6786 loss_val: 1.3818 acc_val: 0.5460 time: 0.0525s\n",
      "Epoch: 0064 loss_train: 1.1192 acc_train: 0.6929 loss_val: 1.3713 acc_val: 0.5510 time: 0.0524s\n",
      "Epoch: 0065 loss_train: 1.1119 acc_train: 0.7000 loss_val: 1.3610 acc_val: 0.5553 time: 0.0525s\n",
      "Epoch: 0066 loss_train: 1.1037 acc_train: 0.6714 loss_val: 1.3508 acc_val: 0.5596 time: 0.0540s\n",
      "Epoch: 0067 loss_train: 1.0974 acc_train: 0.7071 loss_val: 1.3404 acc_val: 0.5646 time: 0.0526s\n",
      "Epoch: 0068 loss_train: 1.0804 acc_train: 0.7071 loss_val: 1.3295 acc_val: 0.5709 time: 0.0526s\n",
      "Epoch: 0069 loss_train: 1.0609 acc_train: 0.6929 loss_val: 1.3188 acc_val: 0.5763 time: 0.0527s\n",
      "Epoch: 0070 loss_train: 1.0375 acc_train: 0.7357 loss_val: 1.3079 acc_val: 0.5849 time: 0.0540s\n",
      "Epoch: 0071 loss_train: 1.0343 acc_train: 0.7357 loss_val: 1.2971 acc_val: 0.5892 time: 0.0530s\n",
      "Epoch: 0072 loss_train: 1.0237 acc_train: 0.7143 loss_val: 1.2869 acc_val: 0.5970 time: 0.0525s\n",
      "Epoch: 0073 loss_train: 1.0481 acc_train: 0.7214 loss_val: 1.2775 acc_val: 0.5974 time: 0.0526s\n",
      "Epoch: 0074 loss_train: 1.0199 acc_train: 0.7429 loss_val: 1.2684 acc_val: 0.6044 time: 0.0537s\n",
      "Epoch: 0075 loss_train: 1.0259 acc_train: 0.7214 loss_val: 1.2598 acc_val: 0.6102 time: 0.0526s\n",
      "Epoch: 0076 loss_train: 0.9828 acc_train: 0.7571 loss_val: 1.2514 acc_val: 0.6176 time: 0.0525s\n",
      "Epoch: 0077 loss_train: 0.9732 acc_train: 0.7429 loss_val: 1.2433 acc_val: 0.6238 time: 0.0526s\n",
      "Epoch: 0078 loss_train: 0.9611 acc_train: 0.8071 loss_val: 1.2354 acc_val: 0.6266 time: 0.0542s\n",
      "Epoch: 0079 loss_train: 0.9405 acc_train: 0.8000 loss_val: 1.2276 acc_val: 0.6301 time: 0.0526s\n",
      "Epoch: 0080 loss_train: 0.9635 acc_train: 0.7643 loss_val: 1.2200 acc_val: 0.6289 time: 0.0527s\n",
      "Epoch: 0081 loss_train: 0.9215 acc_train: 0.8000 loss_val: 1.2122 acc_val: 0.6277 time: 0.0525s\n",
      "Epoch: 0082 loss_train: 0.8908 acc_train: 0.7857 loss_val: 1.2045 acc_val: 0.6285 time: 0.0540s\n",
      "Epoch: 0083 loss_train: 0.9294 acc_train: 0.7786 loss_val: 1.1966 acc_val: 0.6297 time: 0.0526s\n",
      "Epoch: 0084 loss_train: 0.9195 acc_train: 0.7929 loss_val: 1.1895 acc_val: 0.6281 time: 0.0527s\n",
      "Epoch: 0085 loss_train: 0.9280 acc_train: 0.7786 loss_val: 1.1816 acc_val: 0.6297 time: 0.0525s\n",
      "Epoch: 0086 loss_train: 0.8782 acc_train: 0.7786 loss_val: 1.1734 acc_val: 0.6340 time: 0.0539s\n",
      "Epoch: 0087 loss_train: 0.8808 acc_train: 0.7857 loss_val: 1.1645 acc_val: 0.6390 time: 0.0527s\n",
      "Epoch: 0088 loss_train: 0.8582 acc_train: 0.8000 loss_val: 1.1556 acc_val: 0.6452 time: 0.0524s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0089 loss_train: 0.8497 acc_train: 0.8286 loss_val: 1.1477 acc_val: 0.6503 time: 0.0525s\n",
      "Epoch: 0090 loss_train: 0.8580 acc_train: 0.8071 loss_val: 1.1406 acc_val: 0.6546 time: 0.0542s\n",
      "Epoch: 0091 loss_train: 0.8337 acc_train: 0.8286 loss_val: 1.1339 acc_val: 0.6581 time: 0.0525s\n",
      "Epoch: 0092 loss_train: 0.8461 acc_train: 0.8071 loss_val: 1.1271 acc_val: 0.6608 time: 0.0524s\n",
      "Epoch: 0093 loss_train: 0.8362 acc_train: 0.8214 loss_val: 1.1200 acc_val: 0.6651 time: 0.0525s\n",
      "Epoch: 0094 loss_train: 0.8238 acc_train: 0.7929 loss_val: 1.1127 acc_val: 0.6717 time: 0.0538s\n",
      "Epoch: 0095 loss_train: 0.7861 acc_train: 0.8571 loss_val: 1.1058 acc_val: 0.6756 time: 0.0526s\n",
      "Epoch: 0096 loss_train: 0.7720 acc_train: 0.8571 loss_val: 1.0994 acc_val: 0.6787 time: 0.0525s\n",
      "Epoch: 0097 loss_train: 0.7653 acc_train: 0.8286 loss_val: 1.0941 acc_val: 0.6772 time: 0.0525s\n",
      "Epoch: 0098 loss_train: 0.8227 acc_train: 0.8429 loss_val: 1.0901 acc_val: 0.6748 time: 0.0536s\n",
      "Epoch: 0099 loss_train: 0.7704 acc_train: 0.8500 loss_val: 1.0859 acc_val: 0.6748 time: 0.0528s\n",
      "Epoch: 0100 loss_train: 0.7655 acc_train: 0.8286 loss_val: 1.0815 acc_val: 0.6768 time: 0.0527s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 5.3602s\n",
      "Test set results: loss= 1.0815 accuracy= 0.6768\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# evaluating\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKHEyXp1EVdo"
   },
   "source": [
    "# Graph Attention Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx15HdotKnt_"
   },
   "source": [
    "## Graph attention layer (Your task)\n",
    "A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n",
    "\n",
    "\n",
    "### The initial transformation\n",
    "In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n",
    "\n",
    "### attention score\n",
    "We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n",
    "$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n",
    "where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n",
    "We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n",
    "\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n",
    "\n",
    "#### How to vectorize this? Some hints: \n",
    "1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n",
    "\n",
    "2. `tensor.repeat_interleave()` gives you\n",
    "$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n",
    "\n",
    "3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n",
    "\n",
    "4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n",
    "\n",
    "\n",
    "#### Perform softmax \n",
    "First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n",
    "Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n",
    "\n",
    "#### Apply dropout\n",
    "Apply the dropout layer. (this step is easy)\n",
    "\n",
    "#### Calculate final output for each head\n",
    "$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n",
    "\n",
    "\n",
    "#### Concat or Mean\n",
    "Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wVu7rcOuAUZz"
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 alpha: float = 0.2):\n",
    "        \"\"\"\n",
    "        in_features: F, the number of input features per node\n",
    "        out_features: F', the number of output features per node\n",
    "        n_heads: K, the number of attention heads\n",
    "        is_concat: whether the multi-head results should be concatenated or averaged\n",
    "        dropout: the dropout probability\n",
    "        alpha: the negative slope for leaky relu activation\n",
    "        \"\"\"\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # TODO: initialize the following modules: \n",
    "        # (1) self.W: Linear layer that transform the input feature before self attention. \n",
    "        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n",
    "        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n",
    "        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n",
    "        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n",
    "        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n",
    "        ################ your code here ########################\n",
    "        \n",
    "        self.W = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
    "        self.attention = nn.Linear(2 * self.n_hidden, 1, bias = False)\n",
    "        self.activation = nn.LeakyReLU(alpha)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "        ########################################################\n",
    "\n",
    "    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        \n",
    "        # TODO: \n",
    "        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n",
    "        #     (you can use tensor.view() function)\n",
    "        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n",
    "        # (3) apply the attention layer \n",
    "        # (4) apply the activation layer (you will get the attention score e)\n",
    "        # (5) remove the last dimension 1 use tensor.squeeze()\n",
    "        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n",
    "        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n",
    "        # (7) apply softmax \n",
    "        # (8) apply dropout_layer \n",
    "        ############## Your code here #########################################\n",
    "\n",
    "        s = self.W(h)\n",
    "        s = s.view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        s_i = s.repeat(n_nodes, 1, 1)\n",
    "        s_j = s.repeat_interleave(n_nodes, dim = 0)\n",
    "        s_ij = torch.cat((s_i, s_j), dim = -1)\n",
    "        s_ij = s_ij.view(n_nodes, n_nodes, self.n_heads, 2*self.n_hidden)\n",
    "        e = self.attention(s_ij)\n",
    "        e = self.activation(e)\n",
    "        e = e.squeeze(-1)\n",
    "        e = e.masked_fill(adj_mat.unsqueeze(-1) == 0, -np.inf)\n",
    "        e = self.softmax(e)\n",
    "        a = self.dropout_layer(e)\n",
    "\n",
    "\n",
    "        #######################################################################\n",
    "\n",
    "        # Summation \n",
    "        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n",
    "\n",
    "\n",
    "        # TODO: Concat or Mean\n",
    "        # Concatenate the heads\n",
    "        if self.is_concat:\n",
    "            ############## Your code here #########################################\n",
    "            return h_prime.reshape(n_nodes, self.n_heads*self.n_hidden)  \n",
    "            #######################################################################\n",
    "        # Take the mean of the heads (for the last layer)\n",
    "        else:\n",
    "            ############## Your code here #########################################\n",
    "            return h_prime.mean(dim = 1)\n",
    "            #######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOSk_ZShi2nR"
   },
   "source": [
    "## Define GAT network\n",
    "it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jKNbUtPVi1Vs"
   },
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "\n",
    "    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n",
    "        \"\"\"\n",
    "        in_features: the number of features per node\n",
    "        n_hidden: the number of features in the first graph attention layer\n",
    "        n_classes: the number of classes\n",
    "        n_heads: the number of heads in the graph attention layers\n",
    "        dropout: the dropout probability\n",
    "        alpha: the negative input slope for leaky ReLU of the attention layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First graph attention layer where we concatenate the heads\n",
    "        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n",
    "        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n",
    "        self.activation = nn.ELU()  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: the features vectors\n",
    "        adj_mat: the adjacency matrix\n",
    "        \"\"\"\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc1(x, adj_mat)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj_mat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtRQ3Ced7RAw"
   },
   "source": [
    "## training GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b7D5mYXC6zTG"
   },
   "outputs": [],
   "source": [
    "args = {\"training_samples\": 140,\n",
    "        \"epochs\": 100,\n",
    "        \"lr\": 0.01,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"hidden\": 16,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bias\": True, \n",
    "        \"alpha\": 0.2,\n",
    "        \"n_heads\": 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7MYaK98hDy7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "model = GAT(nfeat=features.shape[1],\n",
    "            n_hidden=args[\"hidden\"],\n",
    "            n_classes=labels.max().item() + 1,\n",
    "            dropout=args[\"dropout\"],\n",
    "            alpha=args[\"alpha\"],\n",
    "            n_heads=args[\"n_heads\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n",
    "adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "E9FcfXwMDzEt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9458 acc_train: 0.1357 loss_val: 1.9425 acc_val: 0.4100 time: 6.3308s\n",
      "Epoch: 0002 loss_train: 1.9406 acc_train: 0.4571 loss_val: 1.9387 acc_val: 0.4159 time: 5.9741s\n",
      "Epoch: 0003 loss_train: 1.9358 acc_train: 0.4929 loss_val: 1.9347 acc_val: 0.4132 time: 5.8839s\n",
      "Epoch: 0004 loss_train: 1.9302 acc_train: 0.4500 loss_val: 1.9302 acc_val: 0.4108 time: 5.9255s\n",
      "Epoch: 0005 loss_train: 1.9224 acc_train: 0.4786 loss_val: 1.9254 acc_val: 0.4097 time: 5.4520s\n",
      "Epoch: 0006 loss_train: 1.9122 acc_train: 0.4786 loss_val: 1.9202 acc_val: 0.4019 time: 5.7872s\n",
      "Epoch: 0007 loss_train: 1.9047 acc_train: 0.4857 loss_val: 1.9145 acc_val: 0.3999 time: 5.5226s\n",
      "Epoch: 0008 loss_train: 1.9002 acc_train: 0.4500 loss_val: 1.9085 acc_val: 0.3976 time: 5.2193s\n",
      "Epoch: 0009 loss_train: 1.8909 acc_train: 0.4357 loss_val: 1.9022 acc_val: 0.3937 time: 5.3418s\n",
      "Epoch: 0010 loss_train: 1.8824 acc_train: 0.4429 loss_val: 1.8954 acc_val: 0.3929 time: 6.2284s\n",
      "Epoch: 0011 loss_train: 1.8664 acc_train: 0.4571 loss_val: 1.8882 acc_val: 0.3914 time: 5.5775s\n",
      "Epoch: 0012 loss_train: 1.8608 acc_train: 0.4571 loss_val: 1.8808 acc_val: 0.3921 time: 5.1923s\n",
      "Epoch: 0013 loss_train: 1.8561 acc_train: 0.4286 loss_val: 1.8729 acc_val: 0.3921 time: 5.3570s\n",
      "Epoch: 0014 loss_train: 1.8321 acc_train: 0.4857 loss_val: 1.8647 acc_val: 0.3902 time: 5.6218s\n",
      "Epoch: 0015 loss_train: 1.8290 acc_train: 0.4500 loss_val: 1.8561 acc_val: 0.3879 time: 6.1263s\n",
      "Epoch: 0016 loss_train: 1.8036 acc_train: 0.4214 loss_val: 1.8471 acc_val: 0.3851 time: 5.8396s\n",
      "Epoch: 0017 loss_train: 1.8189 acc_train: 0.4214 loss_val: 1.8379 acc_val: 0.3836 time: 5.6316s\n",
      "Epoch: 0018 loss_train: 1.7868 acc_train: 0.4643 loss_val: 1.8284 acc_val: 0.3808 time: 6.0092s\n",
      "Epoch: 0019 loss_train: 1.7938 acc_train: 0.4214 loss_val: 1.8188 acc_val: 0.3801 time: 5.4297s\n",
      "Epoch: 0020 loss_train: 1.7744 acc_train: 0.4357 loss_val: 1.8090 acc_val: 0.3754 time: 5.2359s\n",
      "Epoch: 0021 loss_train: 1.7745 acc_train: 0.4143 loss_val: 1.7991 acc_val: 0.3738 time: 5.2835s\n",
      "Epoch: 0022 loss_train: 1.7335 acc_train: 0.4286 loss_val: 1.7890 acc_val: 0.3719 time: 5.2506s\n",
      "Epoch: 0023 loss_train: 1.7160 acc_train: 0.4571 loss_val: 1.7788 acc_val: 0.3711 time: 5.8294s\n",
      "Epoch: 0024 loss_train: 1.7325 acc_train: 0.4214 loss_val: 1.7685 acc_val: 0.3719 time: 7.4629s\n",
      "Epoch: 0025 loss_train: 1.6583 acc_train: 0.4857 loss_val: 1.7578 acc_val: 0.3719 time: 6.5894s\n",
      "Epoch: 0026 loss_train: 1.6636 acc_train: 0.4357 loss_val: 1.7470 acc_val: 0.3731 time: 5.5281s\n",
      "Epoch: 0027 loss_train: 1.6242 acc_train: 0.4500 loss_val: 1.7360 acc_val: 0.3750 time: 5.7021s\n",
      "Epoch: 0028 loss_train: 1.6759 acc_train: 0.4429 loss_val: 1.7249 acc_val: 0.3777 time: 5.5583s\n",
      "Epoch: 0029 loss_train: 1.6338 acc_train: 0.4786 loss_val: 1.7138 acc_val: 0.3816 time: 5.1982s\n",
      "Epoch: 0030 loss_train: 1.5779 acc_train: 0.4357 loss_val: 1.7027 acc_val: 0.3859 time: 5.4497s\n",
      "Epoch: 0031 loss_train: 1.6147 acc_train: 0.4429 loss_val: 1.6917 acc_val: 0.3894 time: 5.2207s\n",
      "Epoch: 0032 loss_train: 1.5560 acc_train: 0.4214 loss_val: 1.6806 acc_val: 0.3941 time: 5.2513s\n",
      "Epoch: 0033 loss_train: 1.5111 acc_train: 0.4857 loss_val: 1.6694 acc_val: 0.3976 time: 5.3044s\n",
      "Epoch: 0034 loss_train: 1.5305 acc_train: 0.4786 loss_val: 1.6584 acc_val: 0.4050 time: 6.6764s\n",
      "Epoch: 0035 loss_train: 1.5299 acc_train: 0.4714 loss_val: 1.6472 acc_val: 0.4108 time: 5.5148s\n",
      "Epoch: 0036 loss_train: 1.5515 acc_train: 0.4857 loss_val: 1.6361 acc_val: 0.4151 time: 5.7602s\n",
      "Epoch: 0037 loss_train: 1.4705 acc_train: 0.4786 loss_val: 1.6250 acc_val: 0.4221 time: 5.8606s\n",
      "Epoch: 0038 loss_train: 1.5119 acc_train: 0.4857 loss_val: 1.6139 acc_val: 0.4287 time: 5.9332s\n",
      "Epoch: 0039 loss_train: 1.4870 acc_train: 0.5071 loss_val: 1.6029 acc_val: 0.4369 time: 5.6999s\n",
      "Epoch: 0040 loss_train: 1.4824 acc_train: 0.5000 loss_val: 1.5921 acc_val: 0.4451 time: 5.7162s\n",
      "Epoch: 0041 loss_train: 1.4243 acc_train: 0.5357 loss_val: 1.5811 acc_val: 0.4525 time: 5.2969s\n",
      "Epoch: 0042 loss_train: 1.4253 acc_train: 0.5714 loss_val: 1.5701 acc_val: 0.4630 time: 5.2874s\n",
      "Epoch: 0043 loss_train: 1.4579 acc_train: 0.5714 loss_val: 1.5592 acc_val: 0.4704 time: 5.2555s\n",
      "Epoch: 0044 loss_train: 1.3986 acc_train: 0.5643 loss_val: 1.5485 acc_val: 0.4794 time: 5.2419s\n",
      "Epoch: 0045 loss_train: 1.3808 acc_train: 0.5786 loss_val: 1.5376 acc_val: 0.4903 time: 5.3431s\n",
      "Epoch: 0046 loss_train: 1.3903 acc_train: 0.6143 loss_val: 1.5269 acc_val: 0.5023 time: 5.1862s\n",
      "Epoch: 0047 loss_train: 1.3742 acc_train: 0.6071 loss_val: 1.5162 acc_val: 0.5132 time: 5.2070s\n",
      "Epoch: 0048 loss_train: 1.3453 acc_train: 0.6429 loss_val: 1.5056 acc_val: 0.5284 time: 5.1999s\n",
      "Epoch: 0049 loss_train: 1.3595 acc_train: 0.6143 loss_val: 1.4952 acc_val: 0.5467 time: 5.2899s\n",
      "Epoch: 0050 loss_train: 1.3480 acc_train: 0.6500 loss_val: 1.4850 acc_val: 0.5592 time: 5.2368s\n",
      "Epoch: 0051 loss_train: 1.2783 acc_train: 0.6714 loss_val: 1.4749 acc_val: 0.5728 time: 5.3699s\n",
      "Epoch: 0052 loss_train: 1.3094 acc_train: 0.6286 loss_val: 1.4652 acc_val: 0.5837 time: 5.2164s\n",
      "Epoch: 0053 loss_train: 1.3054 acc_train: 0.6643 loss_val: 1.4554 acc_val: 0.6001 time: 5.2673s\n",
      "Epoch: 0054 loss_train: 1.1831 acc_train: 0.6857 loss_val: 1.4456 acc_val: 0.6114 time: 5.2496s\n",
      "Epoch: 0055 loss_train: 1.2796 acc_train: 0.6643 loss_val: 1.4362 acc_val: 0.6246 time: 5.4591s\n",
      "Epoch: 0056 loss_train: 1.3499 acc_train: 0.6929 loss_val: 1.4268 acc_val: 0.6340 time: 5.2851s\n",
      "Epoch: 0057 loss_train: 1.3443 acc_train: 0.6571 loss_val: 1.4180 acc_val: 0.6452 time: 5.3056s\n",
      "Epoch: 0058 loss_train: 1.1654 acc_train: 0.7571 loss_val: 1.4090 acc_val: 0.6542 time: 5.3041s\n",
      "Epoch: 0059 loss_train: 1.2079 acc_train: 0.7000 loss_val: 1.3999 acc_val: 0.6616 time: 5.3161s\n",
      "Epoch: 0060 loss_train: 1.1576 acc_train: 0.7429 loss_val: 1.3909 acc_val: 0.6694 time: 5.1906s\n",
      "Epoch: 0061 loss_train: 1.1700 acc_train: 0.7571 loss_val: 1.3820 acc_val: 0.6745 time: 5.3190s\n",
      "Epoch: 0062 loss_train: 1.1985 acc_train: 0.7143 loss_val: 1.3730 acc_val: 0.6783 time: 5.2359s\n",
      "Epoch: 0063 loss_train: 1.1405 acc_train: 0.7357 loss_val: 1.3642 acc_val: 0.6807 time: 5.3463s\n",
      "Epoch: 0064 loss_train: 1.1157 acc_train: 0.7500 loss_val: 1.3553 acc_val: 0.6861 time: 5.3429s\n",
      "Epoch: 0065 loss_train: 1.1439 acc_train: 0.7286 loss_val: 1.3461 acc_val: 0.6893 time: 5.2023s\n",
      "Epoch: 0066 loss_train: 1.2047 acc_train: 0.7143 loss_val: 1.3371 acc_val: 0.6908 time: 5.3266s\n",
      "Epoch: 0067 loss_train: 1.1705 acc_train: 0.7429 loss_val: 1.3283 acc_val: 0.6939 time: 6.0358s\n",
      "Epoch: 0068 loss_train: 1.0980 acc_train: 0.7786 loss_val: 1.3190 acc_val: 0.6955 time: 5.4847s\n",
      "Epoch: 0069 loss_train: 1.1257 acc_train: 0.7214 loss_val: 1.3099 acc_val: 0.6967 time: 5.4037s\n",
      "Epoch: 0070 loss_train: 1.1065 acc_train: 0.7286 loss_val: 1.3009 acc_val: 0.6990 time: 5.3524s\n",
      "Epoch: 0071 loss_train: 1.0592 acc_train: 0.7857 loss_val: 1.2917 acc_val: 0.7002 time: 5.2859s\n",
      "Epoch: 0072 loss_train: 1.1566 acc_train: 0.7929 loss_val: 1.2829 acc_val: 0.7013 time: 5.2254s\n",
      "Epoch: 0073 loss_train: 1.0822 acc_train: 0.7571 loss_val: 1.2741 acc_val: 0.7025 time: 5.2233s\n",
      "Epoch: 0074 loss_train: 1.1337 acc_train: 0.7929 loss_val: 1.2657 acc_val: 0.7029 time: 5.5012s\n",
      "Epoch: 0075 loss_train: 1.0477 acc_train: 0.7714 loss_val: 1.2575 acc_val: 0.7052 time: 5.4277s\n",
      "Epoch: 0076 loss_train: 1.0904 acc_train: 0.7786 loss_val: 1.2494 acc_val: 0.7083 time: 5.4124s\n",
      "Epoch: 0077 loss_train: 0.9759 acc_train: 0.7929 loss_val: 1.2417 acc_val: 0.7111 time: 5.1895s\n",
      "Epoch: 0078 loss_train: 1.0155 acc_train: 0.8357 loss_val: 1.2338 acc_val: 0.7130 time: 5.3443s\n",
      "Epoch: 0079 loss_train: 1.0197 acc_train: 0.7571 loss_val: 1.2261 acc_val: 0.7157 time: 5.3666s\n",
      "Epoch: 0080 loss_train: 1.0084 acc_train: 0.7857 loss_val: 1.2185 acc_val: 0.7169 time: 5.3502s\n",
      "Epoch: 0081 loss_train: 0.9554 acc_train: 0.7929 loss_val: 1.2110 acc_val: 0.7192 time: 5.2753s\n",
      "Epoch: 0082 loss_train: 1.0095 acc_train: 0.8286 loss_val: 1.2035 acc_val: 0.7227 time: 5.3555s\n",
      "Epoch: 0083 loss_train: 1.0557 acc_train: 0.7429 loss_val: 1.1962 acc_val: 0.7239 time: 6.2955s\n",
      "Epoch: 0084 loss_train: 0.9870 acc_train: 0.7929 loss_val: 1.1891 acc_val: 0.7251 time: 5.6438s\n",
      "Epoch: 0085 loss_train: 0.9729 acc_train: 0.7714 loss_val: 1.1818 acc_val: 0.7262 time: 5.6094s\n",
      "Epoch: 0086 loss_train: 0.9895 acc_train: 0.7571 loss_val: 1.1746 acc_val: 0.7255 time: 5.2927s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 1.0239 acc_train: 0.7500 loss_val: 1.1674 acc_val: 0.7270 time: 5.2427s\n",
      "Epoch: 0088 loss_train: 1.0002 acc_train: 0.7571 loss_val: 1.1601 acc_val: 0.7282 time: 5.1758s\n",
      "Epoch: 0089 loss_train: 0.9209 acc_train: 0.7786 loss_val: 1.1533 acc_val: 0.7294 time: 5.2427s\n",
      "Epoch: 0090 loss_train: 0.8659 acc_train: 0.8214 loss_val: 1.1464 acc_val: 0.7309 time: 5.1581s\n",
      "Epoch: 0091 loss_train: 0.9652 acc_train: 0.7571 loss_val: 1.1397 acc_val: 0.7309 time: 5.2053s\n",
      "Epoch: 0092 loss_train: 0.8752 acc_train: 0.8357 loss_val: 1.1331 acc_val: 0.7301 time: 5.1917s\n",
      "Epoch: 0093 loss_train: 0.9294 acc_train: 0.7857 loss_val: 1.1268 acc_val: 0.7305 time: 4.7195s\n",
      "Epoch: 0094 loss_train: 0.9193 acc_train: 0.7929 loss_val: 1.1208 acc_val: 0.7317 time: 5.1768s\n",
      "Epoch: 0095 loss_train: 0.9102 acc_train: 0.8429 loss_val: 1.1147 acc_val: 0.7313 time: 5.1801s\n",
      "Epoch: 0096 loss_train: 0.8356 acc_train: 0.8214 loss_val: 1.1087 acc_val: 0.7317 time: 5.3754s\n",
      "Epoch: 0097 loss_train: 0.9578 acc_train: 0.8143 loss_val: 1.1028 acc_val: 0.7333 time: 5.3684s\n",
      "Epoch: 0098 loss_train: 0.8871 acc_train: 0.8143 loss_val: 1.0971 acc_val: 0.7336 time: 5.2498s\n",
      "Epoch: 0099 loss_train: 0.9503 acc_train: 0.7643 loss_val: 1.0915 acc_val: 0.7340 time: 5.2041s\n",
      "Epoch: 0100 loss_train: 0.9233 acc_train: 0.7500 loss_val: 1.0862 acc_val: 0.7360 time: 5.3377s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 551.1840s\n",
      "Test set results: loss= 1.0862 accuracy= 0.7360\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args[\"epochs\"]):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6Ox3fbTG7rc"
   },
   "source": [
    "# Question: (Your task)\n",
    "Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urJ8Q-neDzHU"
   },
   "source": [
    "**Answer:** \n",
    "   \n",
    "I expect that attention mechanism (GAT) provides better results than Vanilla GCN. When the test set results are compared, GAT (73.6%) is better than Vanilla GCN (~67.7%). However, GAT is required much time (or computational power) to complete training. The additional computation in attention mechanism helps to highlight nodes which have high effects on the prediction.  \n",
    "   \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esv21BaOtpOU"
   },
   "source": [
    "# Late Policy\n",
    "You may use up to 7 grace days over the course of the semester for the |practicals you will take. You will only use up to 3 grace days per assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eiab3C-Ztxg_"
   },
   "source": [
    "# Academic Integrity\n",
    "All work on assignments must be done individually unless stated otherwise. Turning in someone else’s work, in whole or in part, as your own will be considered as a violation of academic integrity. Please note that the former condition also holds for the material found on the web as everything on the web has been written by someone else.\n",
    "\n",
    "## Acknowledgements\n",
    "Adapted from University of Toronto, Neural Networks and Deep Learning course (CSC413/2516)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e10f8bc3a9bab29f2213eb58f93eaced6738d931646fd1ebf4918884c5d9b1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
